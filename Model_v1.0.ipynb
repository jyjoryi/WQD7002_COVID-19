{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from keras.layers import GlobalAveragePooling2D, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.applications import resnet50\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import GlorotNormal\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.metrics import Precision, Recall, TruePositives, TrueNegatives, FalsePositives, FalseNegatives\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, auc, roc_auc_score, roc_curve \n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.utils import class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from tabulate import tabulate\n",
    "from numpy import interp\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MYT = pytz.timezone('Asia/Kuala_Lumpur')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now(MYT)\n",
    "print(\"Start Time:\", start_time.strftime('%Y/%m/%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Number of GPU available: \", len(physical_devices))\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "sess = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_regularizer = 1000\n",
    "model_lr = 1e-6\n",
    "model_epochs = 50\n",
    "\n",
    "deeptune_model_regularizer = 100\n",
    "deeptune_model_lr = 1e-7\n",
    "deeptune_model_epochs = 150\n",
    "\n",
    "fold = 1\n",
    "run = f'Fold{fold}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pil_image(img_path, height, width):\n",
    "        with open(img_path, 'rb') as f:\n",
    "            return np.array(Image.open(f).convert('RGB').resize((width, height)))\n",
    "\n",
    "def load_all_images(dataset_path, height, width):\n",
    "    return np.array([read_pil_image(str(p), height, width) for p in Path(dataset_path).rglob(\"*.*\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Cross Validation / Actual Classification Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 4\n",
    "height = width = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = f'../Dataset/Cross_Validation/Fold{fold}/train'\n",
    "val_path = f'../Dataset/Cross_Validation/Fold{fold}/val'\n",
    "test_path = f'../Dataset/Cross_Validation/Fold{fold}/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run these without zero-meaning data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_batch = ImageDataGenerator(preprocessing_function=resnet50.preprocess_input).flow_from_directory(\n",
    "    directory = train_path,\n",
    "    target_size = (224,224),\n",
    "    batch_size = 16\n",
    ")\n",
    "\n",
    "val_batch = ImageDataGenerator(preprocessing_function=resnet50.preprocess_input).flow_from_directory(\n",
    "    directory = val_path,\n",
    "    target_size = (224,224),\n",
    "    batch_size = 16\n",
    ")\n",
    "\n",
    "test_batch = ImageDataGenerator(preprocessing_function=resnet50.preprocess_input).flow_from_directory(\n",
    "    directory = test_path,\n",
    "    target_size = (224,224),\n",
    "    batch_size = 16,\n",
    "    shuffle = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run these to zero-mean data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_start_time = datetime.now(MYT)\n",
    "print(\"Start Time (Hold-out Validation Data Preparation):\", data_prep_start_time.strftime('%Y/%m/%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(preprocessing_function=resnet50.preprocess_input)\n",
    "train_datagen.fit(load_all_images(train_path, height, width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = train_datagen.flow_from_directory(\n",
    "    directory = train_path,\n",
    "    target_size = (height, width),\n",
    "    batch_size = 16\n",
    ")\n",
    "\n",
    "val_batch = train_datagen.flow_from_directory(\n",
    "    directory = val_path,\n",
    "    target_size = (224,224),\n",
    "    batch_size = 16\n",
    ")\n",
    "\n",
    "test_batch = train_datagen.flow_from_directory(\n",
    "    directory = test_path,\n",
    "    target_size = (224,224),\n",
    "    batch_size = 16,\n",
    "    shuffle = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_prep_end_time = datetime.now(MYT)\n",
    "print(\"End Time (Hold-out Validation Data Preparation):\", data_prep_end_time.strftime('%Y/%m/%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Run Time (Hold-out Validation Data Preparation):\", data_prep_end_time-data_prep_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Deep Tuning Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeptuning_num_classes = 2\n",
    "height = width = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeptuning_train_path = '../Dataset/Deep_Tuning/train'\n",
    "deeptuning_val_path = '../Dataset/Deep_Tuning/val'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run these without zero-meaning data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deeptuning_train_batch = ImageDataGenerator(preprocessing_function=resnet50.preprocess_input).flow_from_directory(\n",
    "    directory = deeptuning_train_path,\n",
    "    target_size = (224,224),\n",
    "    batch_size = 5\n",
    ")\n",
    "\n",
    "deeptuning_val_batch = ImageDataGenerator(preprocessing_function=resnet50.preprocess_input).flow_from_directory(\n",
    "    directory = deeptuning_val_path,\n",
    "    target_size = (224,224),\n",
    "    batch_size = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run these to zero-mean data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeptuning_data_prep_start_time = datetime.now(MYT)\n",
    "print(\"Start Time (Deep Tuning Data Preparation):\", deeptuning_data_prep_start_time.strftime('%Y/%m/%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeptuning_train_datagen = ImageDataGenerator(preprocessing_function=resnet50.preprocess_input)\n",
    "deeptuning_train_datagen.fit(load_all_images(deeptuning_train_path, height, width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeptuning_train_batch = deeptuning_train_datagen.flow_from_directory(\n",
    "    directory = deeptuning_train_path,\n",
    "    target_size = (224,224),\n",
    "    batch_size = 64\n",
    ")\n",
    "\n",
    "deeptuning_val_batch = deeptuning_train_datagen.flow_from_directory(\n",
    "    directory = deeptuning_val_path,\n",
    "    target_size = (224,224),\n",
    "    batch_size = 64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeptuning_data_prep_end_time = datetime.now(MYT)\n",
    "print(\"End Time (Deep Tuning Data Preparation):\", deeptuning_data_prep_end_time.strftime('%Y/%m/%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Run Time (Deep Tuning Data Preparation):\", deeptuning_data_prep_end_time-deeptuning_data_prep_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate class weights due to imbalanced dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class_weights = class_weight.compute_class_weight(\n",
    "    'balanced',\n",
    "    np.unique(deeptuning_train_batch.classes),\n",
    "    deeptuning_train_batch.classes\n",
    ")\n",
    "class_weights = {i : class_weights[i] for i in range(deeptuning_num_classes)}\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model1 = resnet50.ResNet50(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = base_model1.output\n",
    "x1 = GlobalAveragePooling2D()(x1)\n",
    "prediction_layer1 = Dense(\n",
    "    num_classes, \n",
    "    activation='softmax', \n",
    "    kernel_regularizer=l2(model_regularizer), \n",
    "    kernel_initializer=GlorotNormal())(x1)\n",
    "model1 = Model(inputs=base_model1.input, outputs=prediction_layer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model1.layers:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(\n",
    "    optimizer = Adam(lr=model_lr),\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_training_start_time = datetime.now(MYT)\n",
    "print(\"Start Time (Model 1 Training):\", model1_training_start_time.strftime('%Y/%m/%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint1 = tf.keras.callbacks.ModelCheckpoint(\n",
    "    f'model1_weights_zeromean_{run}.h5', \n",
    "    monitor='val_accuracy', \n",
    "    verbose=1,\n",
    "    save_best_only=True, \n",
    "    save_weights_only=True,\n",
    "    mode='max', \n",
    "    period=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history1 = model1.fit(\n",
    "    x = train_batch,\n",
    "    validation_data = val_batch,\n",
    "    epochs = model_epochs,\n",
    "    verbose = 2,\n",
    "    callbacks = [checkpoint1]\n",
    ")\n",
    "history1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_training_end_time = datetime.now(MYT)\n",
    "print(\"End Time (Model 1 Training):\", model1_training_end_time.strftime('%Y/%m/%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Run Time (Model 1 Training):\", model1_training_end_time-model1_training_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history1.history['accuracy'])\n",
    "plt.plot(history1.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history1.history['loss'])\n",
    "plt.plot(history1.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.load_weights(f'model1_weights_zeromean_{run}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions1 = model1.predict(x=test_batch, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Tuning Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeptuning_base_model = resnet50.ResNet50(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeptuning_base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeptuning_base_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "deeptuning_x = deeptuning_base_model.output\n",
    "deeptuning_x = GlobalAveragePooling2D()(deeptuning_x)\n",
    "deeptuning_predictions = Dense(\n",
    "    deeptuning_num_classes, \n",
    "    activation='softmax', \n",
    "    kernel_regularizer=l2(deeptune_model_regularizer), \n",
    "    kernel_initializer=GlorotNormal())(deeptuning_x)\n",
    "deeptune_model = Model(inputs=deeptuning_base_model.input, outputs=deeptuning_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeptune_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in deeptuning_base_model.layers:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeptune_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeptune_model.compile(\n",
    "    optimizer = Adam(lr=deeptune_model_lr),\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeptuning_training_start_time = datetime.now(MYT)\n",
    "print(\"Start Time (Deep Tuning Training):\", deeptuning_training_start_time.strftime('%Y/%m/%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeptune_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'deeptuned_model.h5', \n",
    "    monitor='val_accuracy', \n",
    "    verbose=1,\n",
    "    save_best_only=True, \n",
    "    mode='max', \n",
    "    period=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deeptuning_history = deeptune_model.fit(\n",
    "    x = deeptuning_train_batch,\n",
    "    validation_data=deeptuning_val_batch,\n",
    "    epochs = deeptune_model_epochs,\n",
    "    verbose = 2,\n",
    "    callbacks = [deeptune_checkpoint]\n",
    "    #class_weight = class_weights\n",
    ")\n",
    "deeptuning_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeptuning_training_end_time = datetime.now(MYT)\n",
    "print(\"End Time (Deep Tuning Training):\", deeptuning_training_end_time.strftime('%Y/%m/%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Run Time (Deep Tuning Training):\", deeptuning_training_end_time-deeptuning_training_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(deeptuning_history.history['accuracy'])\n",
    "plt.plot(deeptuning_history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(deeptuning_history.history['loss'])\n",
    "plt.plot(deeptuning_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeptune_model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulding Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model2 = load_model('deeptuned_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_model = Model(inputs=base_model2.input, outputs=base_model2.layers[-3].output)\n",
    "temp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = temp_model.output\n",
    "x2 = GlobalAveragePooling2D()(x2)\n",
    "prediction_layer2 = Dense(\n",
    "    num_classes, \n",
    "    activation='softmax', \n",
    "    kernel_regularizer=l2(model_regularizer), \n",
    "    kernel_initializer=GlorotNormal())(x2)\n",
    "model2 = Model(inputs=temp_model.input, outputs=prediction_layer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model2.layers:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(\n",
    "    optimizer = Adam(lr=model_lr),\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_training_start_time = datetime.now(MYT)\n",
    "print(\"Start Time (Model 2 Training):\", model2_training_start_time.strftime('%Y/%m/%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint2 = tf.keras.callbacks.ModelCheckpoint(\n",
    "    f'model2_weights_zeromean_{run}.h5', \n",
    "    monitor='val_accuracy', \n",
    "    verbose=1,\n",
    "    save_best_only=True, \n",
    "    save_weights_only=True,\n",
    "    mode='max', \n",
    "    period=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history2 = model2.fit(\n",
    "    x = train_batch,\n",
    "    validation_data = val_batch,\n",
    "    epochs = model_epochs,\n",
    "    verbose = 2,\n",
    "    callbacks = [checkpoint2]\n",
    ")\n",
    "history2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_training_end_time = datetime.now(MYT)\n",
    "print(\"End Time (Model 2 Training):\", model2_training_end_time.strftime('%Y/%m/%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Run Time (Model 2 Training):\", model2_training_end_time-model2_training_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history2.history['accuracy'])\n",
    "plt.plot(history2.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history2.history['loss'])\n",
    "plt.plot(history2.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.load_weights(f'model2_weights_zeromean_{run}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = model2.predict(x=test_batch, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Evaluation for Model 1 and Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specificity(y_true, y_pred):\n",
    "    m = TrueNegatives()\n",
    "    m.update_state(y_true, y_pred)\n",
    "    final_tn = m.result().numpy()\n",
    "    \n",
    "    n = FalsePositives()\n",
    "    n.update_state(y_true, y_pred)\n",
    "    final_fp = n.result().numpy()\n",
    "    \n",
    "    return final_tn / (final_tn + final_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = test_batch.classes\n",
    "cm_plot_labels = ['Bacterial Pneumonia','COVID','Normal','Viral Pneumonia']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1 Confusion Matrix Construction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm1 = confusion_matrix(y_true=test_labels, y_pred=predictions1.argmax(axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2 Confusion Matrix Construction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm2 = confusion_matrix(y_true=test_labels, y_pred=predictions2.argmax(axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Confusion Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm=cm1, classes=cm_plot_labels, title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm=cm2, classes=cm_plot_labels, title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy, Precision, Recall, F1-Score, Specificity, ROC-AUC Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true=test_labels\n",
    "y_true_binarized=label_binarize(y_true, classes=[0,1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1 Metrics Computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1=predictions1.argmax(axis=-1)\n",
    "y_pred_binarized1=label_binarize(y_pred1, classes=[0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy1 = accuracy_score(y_true, y_pred1)\n",
    "precision1 = precision_score(y_true, y_pred1, average=None)\n",
    "recall1 = recall_score(y_true, y_pred1, average=None)\n",
    "f1_metrics_score1 = f1_score(y_true, y_pred1, average=None)\n",
    "macro_roc_auc_score1 = roc_auc_score(y_true, predictions1, multi_class=\"ovr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specificity_score1 = []\n",
    "roc_auc_metrics_score1 = []\n",
    "\n",
    "for i in range(num_classes):\n",
    "    specificity_score1.append(specificity(y_true_binarized[:,i], y_pred_binarized1[:,i]))\n",
    "    roc_auc_metrics_score1.append(roc_auc_score(y_true_binarized[:,i], predictions1[:,i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2 Metrics Computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2=predictions2.argmax(axis=-1)\n",
    "y_pred_binarized2=label_binarize(y_pred2, classes=[0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy2 = accuracy_score(y_true, y_pred2)\n",
    "precision2 = precision_score(y_true, y_pred2, average=None)\n",
    "recall2 = recall_score(y_true, y_pred2, average=None)\n",
    "f1_metrics_score2 = f1_score(y_true, y_pred2, average=None)\n",
    "macro_roc_auc_score2 = roc_auc_score(y_true, predictions2, multi_class=\"ovr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specificity_score2 = []\n",
    "roc_auc_metrics_score2 = []\n",
    "\n",
    "for i in range(num_classes):\n",
    "    specificity_score2.append(specificity(y_true_binarized[:,i], y_pred_binarized2[:,i]))\n",
    "    roc_auc_metrics_score2.append(roc_auc_score(y_true_binarized[:,i], predictions2[:,i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display Metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['Class', 'Precision', 'Recall', 'F1-Score', 'Specificity', 'ROC-AUC Score']\n",
    "decimal_formatting = '.5f'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [[],[],[],[]]\n",
    "\n",
    "for i in range(num_classes):\n",
    "    data1[i].append(cm_plot_labels[i])\n",
    "    data1[i].append(format(precision1[i], decimal_formatting))\n",
    "    data1[i].append(format(recall1[i], decimal_formatting))\n",
    "    data1[i].append(format(f1_metrics_score1[i], decimal_formatting))\n",
    "    data1[i].append(format(specificity_score1[i], decimal_formatting))\n",
    "    data1[i].append(format(roc_auc_metrics_score1[i], decimal_formatting))\n",
    "\n",
    "print('Model 1 Metrics Summary:')\n",
    "print(f'Accuracy: {format(accuracy1, decimal_formatting)}\\n')\n",
    "print(f'{tabulate(data1, headers=header)}\\n')\n",
    "print(f'Macro-averaged ROC-AUC Score: {format(macro_roc_auc_score1, decimal_formatting)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = [[],[],[],[]]\n",
    "\n",
    "for i in range(num_classes):\n",
    "    data2[i].append(cm_plot_labels[i])\n",
    "    data2[i].append(format(precision2[i], decimal_formatting))\n",
    "    data2[i].append(format(recall2[i], decimal_formatting))\n",
    "    data2[i].append(format(f1_metrics_score2[i], decimal_formatting))\n",
    "    data2[i].append(format(specificity_score2[i], decimal_formatting))\n",
    "    data2[i].append(format(roc_auc_metrics_score2[i], decimal_formatting))\n",
    "\n",
    "print('Model 2 Metrics Summary:')\n",
    "print(f'Accuracy: {format(accuracy2, decimal_formatting)}\\n')\n",
    "print(f'{tabulate(data2, headers=header)}\\n')\n",
    "print(f'Macro-averaged ROC-AUC Score: {format(macro_roc_auc_score2, decimal_formatting)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ROC-AUC Curve for Comparison between Model 1 and Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1 ROC-AUC Curve Computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr1 = dict()\n",
    "tpr1 = dict()\n",
    "roc_auc1 = dict()\n",
    "\n",
    "for i in range(num_classes):\n",
    "    fpr1[i], tpr1[i], _ = roc_curve(y_true_binarized[:, i], predictions1[:, i])\n",
    "    roc_auc1[i] = auc(fpr1[i], tpr1[i])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "all_fpr1 = np.unique(np.concatenate([fpr1[i] for i in range(num_classes)]))\n",
    "mean_tpr1 = np.zeros_like(all_fpr1)\n",
    "for i in range(num_classes):\n",
    "    mean_tpr1 += interp(all_fpr1, fpr1[i], tpr1[i])\n",
    "mean_tpr1 /= num_classes\n",
    "fpr1[\"macro\"] = all_fpr1\n",
    "tpr1[\"macro\"] = mean_tpr1\n",
    "roc_auc1[\"macro\"] = auc(fpr1[\"macro\"], tpr1[\"macro\"])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr1[\"micro\"], tpr1[\"micro\"], _ = roc_curve(y_true_binarized.ravel(), predictions1.ravel())\n",
    "roc_auc1[\"micro\"] = auc(fpr1[\"micro\"], tpr1[\"micro\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2 ROC-AUC Curve Computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr2 = dict()\n",
    "tpr2 = dict()\n",
    "roc_auc2 = dict()\n",
    "\n",
    "for i in range(num_classes):\n",
    "    fpr2[i], tpr2[i], _ = roc_curve(y_true_binarized[:, i], predictions2[:, i])\n",
    "    roc_auc2[i] = auc(fpr2[i], tpr2[i])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "all_fpr2 = np.unique(np.concatenate([fpr2[i] for i in range(num_classes)]))\n",
    "mean_tpr2 = np.zeros_like(all_fpr2)\n",
    "for i in range(num_classes):\n",
    "    mean_tpr2 += interp(all_fpr2, fpr2[i], tpr2[i])\n",
    "mean_tpr2 /= num_classes\n",
    "fpr2[\"macro\"] = all_fpr2\n",
    "tpr2[\"macro\"] = mean_tpr2\n",
    "roc_auc2[\"macro\"] = auc(fpr2[\"macro\"], tpr2[\"macro\"])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr2[\"micro\"], tpr2[\"micro\"], _ = roc_curve(y_true_binarized.ravel(), predictions2.ravel())\n",
    "roc_auc2[\"micro\"] = auc(fpr2[\"micro\"], tpr2[\"micro\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot ROC-AUC Curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wspace=0.3\n",
    "hspace=0.5\n",
    "subtitle_fontsize = 18\n",
    "subtitle_fontweight = 'semibold'\n",
    "maintitle_fontsize = 32\n",
    "maintitle_fontweight = 'bold'\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "for i in range(num_classes):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.subplots_adjust(wspace=wspace, hspace=hspace)\n",
    "    plt.plot(\n",
    "        fpr1[i],\n",
    "        tpr1[i],\n",
    "        label=f'Model 1 (AUC = {roc_auc1[i]:.3f})',\n",
    "        color='navy',\n",
    "        linewidth=2\n",
    "    )\n",
    "    plt.plot(\n",
    "        fpr2[i],\n",
    "        tpr2[i],\n",
    "        label=f'Model 2 (AUC = {roc_auc2[i]:.3f})',\n",
    "        color='darkorange',\n",
    "        linewidth=2\n",
    "    )\n",
    "    plt.title(cm_plot_labels[i], fontsize=subtitle_fontsize, fontweight=subtitle_fontweight)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "plt.subplot(2, 3, 5)\n",
    "plt.subplots_adjust(wspace=wspace, hspace=hspace)\n",
    "plt.plot(\n",
    "    fpr1[\"micro\"],\n",
    "    tpr1[\"micro\"],\n",
    "    label=f'Model 1 (AUC = {roc_auc1[\"micro\"]:.3f})',\n",
    "    color='navy',\n",
    "    linestyle=':',\n",
    "    linewidth=4\n",
    ")\n",
    "plt.plot(\n",
    "    fpr2[\"micro\"],\n",
    "    tpr2[\"micro\"],\n",
    "    label=f'Model 2 (AUC = {roc_auc2[\"micro\"]:.3f})',\n",
    "    color='darkorange',\n",
    "    linestyle=':',\n",
    "    linewidth=4\n",
    ")\n",
    "plt.title('Micro-averaged', fontsize=subtitle_fontsize, fontweight=subtitle_fontweight)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.subplots_adjust(wspace=wspace, hspace=hspace)\n",
    "plt.plot(\n",
    "    fpr1[\"macro\"],\n",
    "    tpr1[\"macro\"],\n",
    "    label=f'Model 1 (AUC = {roc_auc1[\"macro\"]:.3f})',\n",
    "    color='navy',\n",
    "    linestyle=':',\n",
    "    linewidth=4\n",
    ")\n",
    "plt.plot(\n",
    "    fpr2[\"macro\"],\n",
    "    tpr2[\"macro\"],\n",
    "    label=f'Model 2 (AUC = {roc_auc2[\"macro\"]:.3f})',\n",
    "    color='darkorange',\n",
    "    linestyle=':',\n",
    "    linewidth=4\n",
    ")\n",
    "plt.title('Macro-averaged', fontsize=subtitle_fontsize, fontweight=subtitle_fontweight)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.suptitle('ROC Curve', fontsize=maintitle_fontsize, fontweight=maintitle_fontweight)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.now(MYT)\n",
    "print(\"End Time:\", end_time.strftime('%Y/%m/%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Run Time:\", end_time-start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
